{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "twbLWgpO7YQH"
      },
      "source": [
        "## EXERCISE 1: What to do at the airport?\n",
        "\n",
        "You are travelling and have some time to kill at the aiport. There are three things you could spend your time doing:\n",
        "  \n",
        "1) You could have a coffee.\n",
        "\n",
        "This has a probability of $0.8$ of giving you time to relax with a tasty beverage, and a utility of $10$. \n",
        "It also has a probability of $0.2$ of providing you with a nasty cup from over-roasted beans that annoys you,\n",
        "and outcome with a utility of $-5$.\n",
        "\n",
        "2) You could shop for clothes.\n",
        "\n",
        "This has a probability of $0.1$ that you will find a great outfit at a good price, utility $20$. However, it \n",
        "has a probability of $0.9$ that you end up wasting money on over-priced junk, utility $-10$.\n",
        "\n",
        "3) You could have a bite to eat.\n",
        "\n",
        "This has a probability of $0.8$ that you find something rather mediocre that prevents you from being too hungry \n",
        "during your flight, utility $2$, and a probability of $0.2$ that you find something filling and tasty, utility $5$.\n",
        "\n",
        "> __QUESTION 1(a):__ What should you do if you take the principle of maximum expected utility to be your decision criterion?\n",
        "\n",
        "> __QUESTION 1(b):__ What should you do if you take the principle of maximax decision criterion to be your decision criterion?\n",
        "\n",
        "> __QUESTION 1(c):__ What should you do if you take the principle of maximin decision criterion to be your decision criterion?\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MEU: With an EU of 7.0 the best choice is coffee\n",
            "Maximax: With a maximum utility of 20 the best choice is clothes\n",
            "Maximin: With a minimum utility of 2 the best choice is eat\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "clothes_outcomes = ['nice', 'not nice']\n",
        "prob_coffee_outcomes = np.array([0.8, 0.2])\n",
        "util_coffee_outcomes = np.array([10, -5])\n",
        "\n",
        "clothes_outcomes = ['great', 'not great']\n",
        "prob_clothes_outcomes = np.array([0.1, 0.9])\n",
        "util_clothes_outcomes = np.array([20, -10])\n",
        "\n",
        "eat_outcomes = ['alright', 'tasty']\n",
        "prob_eat_outcomes = np.array([0.8, 0.2])\n",
        "util_eat_outcomes = np.array([2, 5])\n",
        "\n",
        "# MEU\n",
        "eu_coffee_outcomes = prob_coffee_outcomes * util_coffee_outcomes\n",
        "eu_coffee = np.sum(eu_coffee_outcomes)\n",
        "eu_clothes_outcomes = prob_clothes_outcomes * util_clothes_outcomes\n",
        "eu_clothes = np.sum(eu_clothes_outcomes)\n",
        "eu_eat_outcomes = prob_eat_outcomes * util_eat_outcomes\n",
        "eu_eat = np.sum(eu_eat_outcomes)\n",
        "\n",
        "eu = np.array([eu_coffee, eu_clothes, eu_eat])\n",
        "max_eu = np.max(eu)\n",
        "action = np.argmax(eu)\n",
        "print(\"MEU: With an EU of\", max_eu, \"the best choice is\", end=' ')\n",
        "if action==0:\n",
        "    print('coffee')\n",
        "elif action==1:\n",
        "    print('clothes')\n",
        "elif action==2:\n",
        "    print('eat')\n",
        "\n",
        "# maximax\n",
        "max_u_coffee_outcome = np.max(util_coffee_outcomes)\n",
        "max_u_clothes_outcome = np.max(util_clothes_outcomes)\n",
        "max_u_eat_outcome = np.max(util_eat_outcomes)\n",
        "u_max = np.array([max_u_coffee_outcome, max_u_clothes_outcome, max_u_eat_outcome])\n",
        "max_u = np.max(u_max)\n",
        "action = np.argmax(u_max)\n",
        "print(\"Maximax: With a maximum utility of\", max_u, \"the best choice is\", end=' ')\n",
        "if action==0:\n",
        "    print('coffee')\n",
        "elif action==1:\n",
        "    print('clothes')\n",
        "elif action==2:\n",
        "    print('eat')\n",
        "\n",
        "# maximin\n",
        "min_u_coffee_outcome = np.min(util_coffee_outcomes)\n",
        "min_u_clothes_outcome = np.min(util_clothes_outcomes)\n",
        "min_u_eat_outcome = np.min(util_eat_outcomes)\n",
        "u_min = np.array([min_u_coffee_outcome, min_u_clothes_outcome, min_u_eat_outcome])\n",
        "max_u = np.max(u_min)\n",
        "action = np.argmax(u_min)\n",
        "print(\"Maximin: With a minimum utility of\", max_u, \"the best choice is\", end=' ')\n",
        "if action==0:\n",
        "    print('coffee')\n",
        "elif action==1:\n",
        "    print('clothes')\n",
        "elif action==2:\n",
        "    print('eat')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wRLlnsWJ90DZ"
      },
      "source": [
        "## EXERCISE 2: Solving a MDP with MDP toolbox\n",
        "\n",
        "We have four states and four actions.\n",
        "\n",
        "The actions are: 0 is Right, 1 is Left, 2 is Up and 3 is Down.\n",
        "\n",
        "The states are 0, 1, 2, 3, and they are arranged like this:\n",
        "    \n",
        "$$\n",
        "\\begin{array}{cc}\n",
        "2 & 3\\\\\n",
        "0 & 1\\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The motion model provides:\n",
        "*   0.8 probability of moving in the direction of the action,\n",
        "*   0.1 probability of moving in each of the directions perpendicular to that of the action.\n",
        "\n",
        "So that 2 is Up from 0 and 1 is Right of 0, and so on. The cost of any action (in any state) is -0.04.\n",
        "\n",
        "In case of \"infeasible\" movements, the agent remains in the current state.\n",
        "\n",
        "The reward for state 3 is 1, and the reward for state 1 is -1, and the agent does not leave those states.\n",
        "\n",
        "Set discount factor equal to 0.99.\n",
        "\n",
        "> __QUESTION 2(a):__ What is the policy based on the Value iteration algorithm?\n",
        "\n",
        "> __QUESTION 2(b):__ What is the policy based on the Policy iteration algorithm?\n",
        "\n",
        "> __QUESTION 2(c):__ What is the policy based on the Q-Learning algorithm?\n",
        "\n",
        "> __QUESTION 2(d):__ Look at the **setVerbose**() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations (hint: see the iter attribute) and the CPU time used to come up with a solution (hint: see the time attribute) in the Value iteration algorithm and Policy iteration algorithm resolutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALUE ITERATION\n",
            "Values:\n",
            " (9.171222983159206, 10.323895223290556, 10.323895223290556, 10.466174574128356)\n",
            "Policy:\n",
            " (2, 2, 0, 0)\n",
            "POLICY ITERATION\n",
            "Values:\n",
            " (98.70501608641325, 99.85770986965021, 99.85770986965022, 99.99999999999991)\n",
            "Policy:\n",
            " (2, 2, 0, 0)\n",
            "QLEARNING\n",
            "Values:\n",
            " (42.06368027504703, 68.42538946292196, 65.64772891492544, 79.02605048260297)\n",
            "Policy:\n",
            " (0, 2, 0, 0)\n",
            "\n",
            "ITERATIONS AND CPU:\n",
            "Value Iteration: 11 ; 0.0010170936584472656\n",
            "Policy Iteration: 2 ; 0.0019989013671875\n"
          ]
        }
      ],
      "source": [
        "import mdptoolbox\n",
        "import numpy as np\n",
        "\n",
        "# azione, stato partenza, stato arrivo\n",
        "# azioni: 0 1 2 3 (right, left, up, down)\n",
        "# stati: 0 1 2 3\n",
        "P1 = np.array([\n",
        "    [[0.1,0.8,0.1,0], [0,1,0,0], [0.1,0,0.1,0.8], [0,0,0,1]],\n",
        "    [[1,0,0,0], [0.8,0.1,0,0.1], [0,0,1,0], [0,0.1,0.8,0.1]],\n",
        "    [[0.1,0.1,0.8,0], [0.1,0.1,0,0.8], [0,0,1,0], [0,0,0,1]],\n",
        "    [[1,0,0,0], [0,1,0,0], [0.8,0,0.1,0.1], [0,0.8,0.1,0.1]]\n",
        "])\n",
        "\n",
        "# stato, azione (premio per fare azione in stato)\n",
        "R1 = np.array([[-1,-0.04,-0.04,-0.04], [-1,-0.04,1,-1], [1,-0.04,-0.04,-0.04], [1,-0.04,1,-1]])\n",
        "\n",
        "mdptoolbox.util.check(P1, R1)\n",
        "\n",
        "vi1 = mdptoolbox.mdp.ValueIteration(P1, R1, 0.99)\n",
        "#vi1.setVerbose()\n",
        "vi1.run()\n",
        "# We can then display the values (utilities) computed, and look at the policy:\n",
        "print(\"VALUE ITERATION\")\n",
        "print('Values:\\n', vi1.V)\n",
        "print('Policy:\\n', vi1.policy)\n",
        "\n",
        "pi1 = mdptoolbox.mdp.PolicyIteration(P1, R1, 0.99)\n",
        "#pi1.setVerbose()\n",
        "pi1.run()\n",
        "# We can then display the values (utilities) computed, and look at the policy:\n",
        "print(\"POLICY ITERATION\")\n",
        "print('Values:\\n', pi1.V)\n",
        "print('Policy:\\n', pi1.policy)\n",
        "\n",
        "ql1 = mdptoolbox.mdp.QLearning(P1, R1, 0.99)\n",
        "ql1.run()\n",
        "# We can then display the values (utilities) computed, and look at the policy:\n",
        "print(\"QLEARNING\")\n",
        "print('Values:\\n', ql1.V)\n",
        "print('Policy:\\n', ql1.policy)\n",
        "\n",
        "print(\"\\nITERATIONS AND CPU:\")\n",
        "print('Value Iteration:', vi1.iter, ';', vi1.time)\n",
        "print('Policy Iteration:', pi1.iter, ';', pi1.time)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
